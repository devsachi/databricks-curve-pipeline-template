{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3480d92",
   "metadata": {},
   "source": [
    "# Curve Formation Pipeline Orchestrator\n",
    "\n",
    "This notebook orchestrates the end-to-end curve formation pipeline, including:\n",
    "1. Market data ingestion and validation\n",
    "2. Curve calculation for different asset classes\n",
    "3. Data quality checks and monitoring\n",
    "4. Output persistence and notifications\n",
    "\n",
    "## Parameters\n",
    "- `env`: Environment (dev/staging/prod)\n",
    "- `trade_date`: Trade date for curve construction (YYYY-MM-DD)\n",
    "- `asset_classes`: List of asset classes to process (comma-separated)\n",
    "- `notification_email`: Email for notifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842199b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curve Formation Pipeline Orchestrator\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.curve_formation.core.processor import CurveProcessor\n",
    "from src.curve_formation.core.writer import CurveWriter\n",
    "from src.utils.config_manager import ConfigManager\n",
    "from src.utils.data_quality.validator import DataValidator\n",
    "from src.utils.exception_handler import handle_exception\n",
    "from src.utils.logging_utils import get_logger\n",
    "from src.utils.spark_utils import get_spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f788e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging and configuration\n",
    "logger = get_logger(__name__)\n",
    "config_manager = ConfigManager()\n",
    "\n",
    "# Get pipeline parameters\n",
    "dbutils.widgets.text(\"env\", \"dev\", \"Environment\")\n",
    "dbutils.widgets.text(\"trade_date\", datetime.now().strftime(\"%Y-%m-%d\"), \"Trade Date\")\n",
    "dbutils.widgets.text(\"asset_classes\", \"IR,FX,CREDIT\", \"Asset Classes\")\n",
    "dbutils.widgets.text(\"notification_email\", \"\", \"Notification Email\")\n",
    "\n",
    "env = dbutils.widgets.get(\"env\")\n",
    "trade_date = datetime.strptime(dbutils.widgets.get(\"trade_date\"), \"%Y-%m-%d\")\n",
    "asset_classes = dbutils.widgets.get(\"asset_classes\").split(\",\")\n",
    "notification_email = dbutils.widgets.get(\"notification_email\")\n",
    "\n",
    "# Load configuration\n",
    "config = config_manager.load_config(env)\n",
    "logger.info(f\"Loaded configuration for environment: {env}\")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = get_spark_session(config)\n",
    "logger.info(\"Initialized Spark session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09743aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and validate market data\n",
    "@handle_exception\n",
    "def load_market_data(trade_date: datetime, asset_classes: list) -> dict:\n",
    "    \"\"\"Load market data for specified asset classes and validate\"\"\"\n",
    "    logger.info(f\"Loading market data for {trade_date} - Asset Classes: {asset_classes}\")\n",
    "    \n",
    "    data_validator = DataValidator()\n",
    "    market_data = {}\n",
    "    \n",
    "    for asset_class in asset_classes:\n",
    "        # Load market data from Delta table\n",
    "        df = spark.table(f\"{config['catalog']}.{config['schemas']['market_data']}.market_data\") \\\n",
    "            .filter(f\"trade_date = '{trade_date.strftime('%Y-%m-%d')}' AND asset_class = '{asset_class}'\")\n",
    "        \n",
    "        # Validate data quality\n",
    "        validation_config = config['data_quality'][asset_class]\n",
    "        if not data_validator.validate_market_data(df, validation_config):\n",
    "            raise ValueError(f\"Data quality validation failed for {asset_class}\")\n",
    "            \n",
    "        market_data[asset_class] = df\n",
    "        logger.info(f\"Successfully loaded and validated market data for {asset_class}\")\n",
    "    \n",
    "    return market_data\n",
    "\n",
    "# Load market data for processing\n",
    "market_data = load_market_data(trade_date, asset_classes)\n",
    "logger.info(\"Market data loading complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7cb6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process curves for each asset class\n",
    "@handle_exception\n",
    "def process_curves(market_data: dict, trade_date: datetime) -> dict:\n",
    "    \"\"\"Construct curves for each asset class\"\"\"\n",
    "    logger.info(\"Starting curve construction process\")\n",
    "    \n",
    "    curve_processor = CurveProcessor(config)\n",
    "    curve_writer = CurveWriter(config)\n",
    "    processed_curves = {}\n",
    "    \n",
    "    for asset_class, data in market_data.items():\n",
    "        logger.info(f\"Processing curves for {asset_class}\")\n",
    "        \n",
    "        # Construct curves\n",
    "        curves = curve_processor.process(\n",
    "            data,\n",
    "            asset_class=asset_class,\n",
    "            trade_date=trade_date\n",
    "        )\n",
    "        \n",
    "        # Validate curve outputs\n",
    "        data_validator = DataValidator()\n",
    "        validation_config = config['data_quality'][f'{asset_class}_curves']\n",
    "        \n",
    "        for curve_name, curve_data in curves.items():\n",
    "            if not data_validator.validate_market_data(curve_data, validation_config):\n",
    "                raise ValueError(f\"Curve validation failed for {curve_name}\")\n",
    "        \n",
    "        # Write curves to Delta table\n",
    "        curve_writer.write_curves(curves, asset_class, trade_date)\n",
    "        processed_curves[asset_class] = curves\n",
    "        \n",
    "        logger.info(f\"Successfully processed and stored curves for {asset_class}\")\n",
    "    \n",
    "    return processed_curves\n",
    "\n",
    "# Process curves\n",
    "processed_curves = process_curves(market_data, trade_date)\n",
    "logger.info(\"Curve processing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72980618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate monitoring metrics and send notifications\n",
    "@handle_exception\n",
    "def generate_monitoring_metrics(processed_curves: dict, trade_date: datetime) -> None:\n",
    "    \"\"\"Generate monitoring metrics and send notifications\"\"\"\n",
    "    logger.info(\"Generating monitoring metrics\")\n",
    "    \n",
    "    metrics = {}\n",
    "    for asset_class, curves in processed_curves.items():\n",
    "        metrics[asset_class] = {\n",
    "            'curve_count': len(curves),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'trade_date': trade_date.strftime('%Y-%m-%d')\n",
    "        }\n",
    "        \n",
    "        # Calculate curve-specific metrics\n",
    "        for curve_name, curve_data in curves.items():\n",
    "            curve_metrics = {\n",
    "                'points_count': curve_data.count(),\n",
    "                'min_tenor': curve_data.agg({'tenor': 'min'}).collect()[0][0],\n",
    "                'max_tenor': curve_data.agg({'tenor': 'max'}).collect()[0][0],\n",
    "                'min_value': curve_data.agg({'value': 'min'}).collect()[0][0],\n",
    "                'max_value': curve_data.agg({'value': 'max'}).collect()[0][0]\n",
    "            }\n",
    "            metrics[asset_class][curve_name] = curve_metrics\n",
    "    \n",
    "    # Write metrics to monitoring table\n",
    "    metrics_df = spark.createDataFrame([metrics])\n",
    "    metrics_df.write.format('delta').mode('append').saveAsTable(\n",
    "        f\"{config['catalog']}.{config['schemas']['monitoring']}.curve_metrics\"\n",
    "    )\n",
    "    \n",
    "    # Send notification if configured\n",
    "    if notification_email:\n",
    "        send_notification(\n",
    "            notification_email,\n",
    "            f\"Curve Formation Pipeline Complete - {trade_date.strftime('%Y-%m-%d')}\",\n",
    "            f\"Processed curves for asset classes: {', '.join(processed_curves.keys())}\\n\"\n",
    "            f\"Metrics: {metrics}\"\n",
    "        )\n",
    "    \n",
    "    logger.info(\"Monitoring metrics generated and notifications sent\")\n",
    "\n",
    "# Generate metrics and send notifications\n",
    "generate_monitoring_metrics(processed_curves, trade_date)\n",
    "logger.info(\"Pipeline execution complete\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
