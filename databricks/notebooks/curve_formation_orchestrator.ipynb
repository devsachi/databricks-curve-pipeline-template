{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3480d92",
   "metadata": {},
   "source": [
    "# Curve Formation Pipeline Orchestrator\n",
    "\n",
    "This notebook orchestrates the end-to-end curve formation pipeline, including:\n",
    "1. Market data ingestion and validation\n",
    "2. Curve calculation for different asset classes\n",
    "3. Data quality checks and monitoring\n",
    "4. Output persistence and notifications\n",
    "\n",
    "## Parameters\n",
    "- `env`: Environment (dev/staging/prod)\n",
    "- `trade_date`: Trade date for curve construction (YYYY-MM-DD)\n",
    "- `asset_classes`: List of asset classes to process (comma-separated)\n",
    "- `notification_email`: Email for notifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842199b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curve Formation Pipeline Orchestrator\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Any\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.curve_formation.core import processor, writer\n",
    "from src.utils import config_manager\n",
    "from src.utils.data_quality import validator\n",
    "from src.utils.exception_handler import handle_exception\n",
    "from src.utils.logging_utils import get_logger\n",
    "from src.utils.spark_utils import create_spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Databricks utilities\n",
    "try:\n",
    "    from pyspark.dbutils import DBUtils\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # Create a new SparkSession if running locally\n",
    "    if 'spark' not in locals():\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # Initialize dbutils if not already available\n",
    "    if 'dbutils' not in locals():\n",
    "        dbutils = DBUtils(spark)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize Databricks utilities: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f788e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging and configuration\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "try:\n",
    "    # Get pipeline parameters using Databricks widgets\n",
    "    dbutils.widgets.text(\"env\", \"dev\", \"Environment\")\n",
    "    dbutils.widgets.text(\"trade_date\", datetime.now().strftime(\"%Y-%m-%d\"), \"Trade Date\")\n",
    "    dbutils.widgets.text(\"asset_classes\", \"IR,FX,CREDIT\", \"Asset Classes\")\n",
    "    dbutils.widgets.text(\"notification_email\", \"\", \"Notification Email\")\n",
    "\n",
    "    env = dbutils.widgets.get(\"env\")\n",
    "    trade_date = datetime.strptime(dbutils.widgets.get(\"trade_date\"), \"%Y-%m-%d\")\n",
    "    asset_classes = dbutils.widgets.get(\"asset_classes\").split(\",\")\n",
    "    notification_email = dbutils.widgets.get(\"notification_email\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Could not initialize Databricks widgets: {str(e)}\")\n",
    "    # Use default values if widgets are not available\n",
    "    env = \"dev\"\n",
    "    trade_date = datetime.now()\n",
    "    asset_classes = [\"IR\", \"FX\", \"CREDIT\"]\n",
    "    notification_email = \"\"\n",
    "\n",
    "# Load configuration\n",
    "config = config_manager.load_config(env)\n",
    "logger.info(f\"Loaded configuration for environment: {env}\")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = create_spark_session(f\"Curve Formation - {env}\")\n",
    "logger.info(\"Initialized Spark session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09743aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and validate market data\n",
    "@handle_exception(\"Error loading market data\")\n",
    "def load_market_data(trade_date: datetime, asset_classes: list[str], config: dict[str, Any]) -> dict[str, DataFrame]:\n",
    "    \"\"\"Load market data for specified asset classes and validate\n",
    "    \n",
    "    Args:\n",
    "        trade_date: Date for which to load market data\n",
    "        asset_classes: List of asset classes to process\n",
    "        config: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping asset class to market data DataFrame\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If data quality validation fails\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading market data for {trade_date} - Asset Classes: {asset_classes}\")\n",
    "    \n",
    "    market_data = {}\n",
    "    \n",
    "    for asset_class in asset_classes:\n",
    "        # Load market data from Delta table\n",
    "        df = spark.table(f\"{config['catalog']}.{config['schemas']['market_data']}.market_data\") \\\n",
    "            .filter(f\"trade_date = '{trade_date.strftime('%Y-%m-%d')}' AND asset_class = '{asset_class}'\")\n",
    "        \n",
    "        # Validate data quality\n",
    "        validation_config = config['data_quality'][asset_class]\n",
    "        if not validator.validate_market_data(df, validation_config):\n",
    "            raise ValueError(f\"Data quality validation failed for {asset_class}\")\n",
    "            \n",
    "        market_data[asset_class] = df\n",
    "        logger.info(f\"Successfully loaded and validated market data for {asset_class}\")\n",
    "    \n",
    "    return market_data\n",
    "\n",
    "# Load market data for processing\n",
    "market_data = load_market_data(trade_date, asset_classes, config)\n",
    "logger.info(\"Market data loading complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7cb6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process curves for each asset class\n",
    "@handle_exception(\"Error processing curves\")\n",
    "def process_curves(spark: SparkSession, market_data: dict[str, DataFrame], trade_date: datetime, config: dict[str, Any]) -> dict[str, dict[str, DataFrame]]:\n",
    "    \"\"\"Construct curves for each asset class\n",
    "    \n",
    "    Args:\n",
    "        spark: Spark session\n",
    "        market_data: Dictionary mapping asset class to market data DataFrame\n",
    "        trade_date: Date for curve construction\n",
    "        config: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Nested dictionary mapping asset class to curve name to curve DataFrame\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If curve validation fails\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting curve construction process\")\n",
    "    \n",
    "    processed_curves = {}\n",
    "    \n",
    "    for asset_class, data in market_data.items():\n",
    "        logger.info(f\"Processing curves for {asset_class}\")\n",
    "        \n",
    "        # Construct curves\n",
    "        curves = processor.process_all_curves(\n",
    "            spark,\n",
    "            {asset_class: data},\n",
    "            config\n",
    "        )\n",
    "        \n",
    "        # Validate curve outputs\n",
    "        validation_config = config['data_quality'][f'{asset_class}_curves']\n",
    "        \n",
    "        for curve_name, curve_data in curves.items():\n",
    "            if not validator.validate_market_data(curve_data, validation_config):\n",
    "                raise ValueError(f\"Curve validation failed for {curve_name}\")\n",
    "        \n",
    "        # Write curves to Delta table\n",
    "        for curve_name, curve_df in curves.items():\n",
    "            table_name = f\"{config['catalog']}.{config['schemas']['curves']}.{curve_name}_{trade_date.strftime('%Y%m%d')}\"\n",
    "            writer.write_output(spark, curve_df, table_name)\n",
    "            \n",
    "        processed_curves[asset_class] = curves\n",
    "        logger.info(f\"Successfully processed and stored curves for {asset_class}\")\n",
    "    \n",
    "    return processed_curves\n",
    "\n",
    "# Process curves\n",
    "processed_curves = process_curves(spark, market_data, trade_date, config)\n",
    "logger.info(\"Curve processing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72980618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate monitoring metrics and send notifications\n",
    "@handle_exception(\"Error generating monitoring metrics\")\n",
    "def generate_monitoring_metrics(spark: SparkSession, processed_curves: dict[str, dict[str, DataFrame]], trade_date: datetime, config: dict[str, Any]) -> None:\n",
    "    \"\"\"Generate monitoring metrics and send notifications\n",
    "    \n",
    "    Args:\n",
    "        spark: Spark session\n",
    "        processed_curves: Nested dictionary mapping asset class to curve name to curve DataFrame\n",
    "        trade_date: Date of curve construction\n",
    "        config: Configuration dictionary\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating monitoring metrics\")\n",
    "    \n",
    "    def calculate_curve_metrics(curve_df: DataFrame) -> dict[str, float]:\n",
    "        \"\"\"Pure function to calculate metrics for a single curve\n",
    "        \n",
    "        Args:\n",
    "            curve_df: DataFrame containing curve data\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing calculated metrics\n",
    "        \"\"\"\n",
    "        stats = curve_df.agg(\n",
    "            F.min('tenor').alias('min_tenor'),\n",
    "            F.max('tenor').alias('max_tenor'),\n",
    "            F.min('value').alias('min_value'),\n",
    "            F.max('value').alias('max_value'),\n",
    "            F.count('*').alias('points_count')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        return {\n",
    "            'points_count': stats['points_count'],\n",
    "            'min_tenor': float(stats['min_tenor']),\n",
    "            'max_tenor': float(stats['max_tenor']),\n",
    "            'min_value': float(stats['min_value']),\n",
    "            'max_value': float(stats['max_value'])\n",
    "        }\n",
    "    \n",
    "    metrics = {}\n",
    "    for asset_class, curves in processed_curves.items():\n",
    "        metrics[asset_class] = {\n",
    "            'curve_count': len(curves),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'trade_date': trade_date.strftime('%Y-%m-%d')\n",
    "        }\n",
    "        \n",
    "        # Calculate curve-specific metrics using pure function\n",
    "        metrics[asset_class].update({\n",
    "            curve_name: calculate_curve_metrics(curve_data)\n",
    "            for curve_name, curve_data in curves.items()\n",
    "        })\n",
    "    \n",
    "    # Write metrics to monitoring table\n",
    "    metrics_df = spark.createDataFrame([metrics])\n",
    "    metrics_df.write.format('delta').mode('append').saveAsTable(\n",
    "        f\"{config['catalog']}.{config['schemas']['monitoring']}.curve_metrics\"\n",
    "    )\n",
    "    \n",
    "    # Send notification if configured\n",
    "    if notification_email:\n",
    "        send_notification(\n",
    "            notification_email,\n",
    "            f\"Curve Formation Pipeline Complete - {trade_date.strftime('%Y-%m-%d')}\",\n",
    "            f\"Processed curves for asset classes: {', '.join(processed_curves.keys())}\\n\"\n",
    "            f\"Metrics: {metrics}\"\n",
    "        )\n",
    "    \n",
    "    logger.info(\"Monitoring metrics generated and notifications sent\")\n",
    "\n",
    "# Generate metrics and send notifications\n",
    "generate_monitoring_metrics(spark, processed_curves, trade_date, config)\n",
    "logger.info(\"Pipeline execution complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d638822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_notification(email: str, subject: str, body: str) -> None:\n",
    "    \"\"\"Send email notification\n",
    "    \n",
    "    Args:\n",
    "        email: Recipient email address\n",
    "        subject: Email subject\n",
    "        body: Email body text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Here you would implement your email sending logic\n",
    "        # For example using AWS SES, SMTP, or other email service\n",
    "        logger.info(f\"Sending notification to {email}\")\n",
    "        logger.info(f\"Subject: {subject}\")\n",
    "        logger.info(f\"Body: {body}\")\n",
    "        # TODO: Implement actual email sending\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to send notification: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
